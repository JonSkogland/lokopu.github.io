[
  {
    "objectID": "posts/Day 1 - 1000 days of coding.html",
    "href": "posts/Day 1 - 1000 days of coding.html",
    "title": "Day 1 - Linear regression on numerical data",
    "section": "",
    "text": "Today marks the first day of a thousand days of coding.\nHaven taken numerous certificates/courses in computer science and machine learning, I’ve come to the realisation (an obvious one at that) that nothing beats daily project based practice. Exercises from courses rarely cement themselves as it’s easy to merely follow instructions without thoroughly problemsolving.\nToday, day 1, I created a linear regression model on a dataset from Kaggle. Although it’s very minimal and not accurate at all, it is a good first.\nThe jupyter notebook can be viewed here:"
  },
  {
    "objectID": "posts/Day 7 - price optimization.html",
    "href": "posts/Day 7 - price optimization.html",
    "title": "Day 7 - Price optimization",
    "section": "",
    "text": "Tried my hand at price optimization - although heavily based on this guide: https://medium.com/operations-research-bit/a-practical-guide-to-pricing-optimisation-using-machine-learning-5ec4bf7f0d4c - as I have never done price optimization before.\nThe jupyter notebook can be viewed here:"
  },
  {
    "objectID": "posts/Day 6 - Multivariate Linear regression.html",
    "href": "posts/Day 6 - Multivariate Linear regression.html",
    "title": "Day 6 - multivariate linear regression",
    "section": "",
    "text": "Today I wanted to try multivariate linear regression\nI did so by using a dataset from the book Statistical Learning with Python, which included a few features of advertising expenditure in relation to sales figures.\nThe linear regression is however quite superficial, and does not go beyond fitting and evaluating model based on mean squared error, as I need to read through more text books on the subject to advance my machine learning skillset.\nThe jupyter notebook can be viewed here:"
  },
  {
    "objectID": "posts/Day 5 - Web scraping.html",
    "href": "posts/Day 5 - Web scraping.html",
    "title": "Day 5 - Web scraping",
    "section": "",
    "text": "Today I wanted to try my hand at web scraping to improve my data fetching skills.\nI did so by scraping all the name and addresses of Rema1000 stores across data and inserting them into a DataFrame.\nThe jupyter notebook can be viewed here:"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Jun 17, 2025 - Day 7 - Price optimization\n\n\n Jun 16, 2025 - Day 6 - multivariate linear regression\n\n\n Jun 15, 2025 - Day 5 - Web scraping\n\n\n Jun 14, 2025 - Day 4 - API and SQL\n\n\n Jun 13, 2025 - Day 3 - data wrangling and plotting\n\n\n Jun 12, 2025 - Day 2 - Linear regression on big mac price vs GDP\n\n\n Jun 11, 2025 - Day 1 - Linear regression on numerical data\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Jon Skogland.\nThis is my personal page, where I’m going to share thoughts, learnings and things that I pursue.  Click the Posts link to see my posts  You can follow me here:\n\nMy Github: @lokopu"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jon Skogland",
    "section": "",
    "text": "This is my personal page, where I’m going to share thoughts, learnings and things that I pursue.  Click the above Posts link to see my recent posts  You can follow me here:\n\nMy Github: @JonSkogland"
  },
  {
    "objectID": "posts/Day 2  - 1000 days of coding.html",
    "href": "posts/Day 2  - 1000 days of coding.html",
    "title": "Day 2 - Linear regression on big mac price vs GDP",
    "section": "",
    "text": "Today I used a datasetfrom The Economist of big mac prices and GDP across countries. I used this data to create a linear regression model to predict big mac prices based on GDP.\nAgain, like yesterday, I used Scikit-Learn and their linear regression model. As I’m still new to this space, I used guidance and code from this data science book.\nThe jupyter notebook can be viewed here:"
  },
  {
    "objectID": "posts/Day 3 - data wrangling and plotting.html",
    "href": "posts/Day 3 - data wrangling and plotting.html",
    "title": "Day 3 - data wrangling and plotting",
    "section": "",
    "text": "Today I focused on using pandas and plotting using altair to improve those skills. The dataset of the day is again from the economist and can be viewed here\nToday was a bit easier compared to the previous days as they lack machine learning usage.\nThe jupyter notebook can be viewed here:"
  },
  {
    "objectID": "posts/Day 4 - API and SQL.html",
    "href": "posts/Day 4 - API and SQL.html",
    "title": "Day 4 - API and SQL",
    "section": "",
    "text": "Today I wanted to try loading data from an API and inserting it into a database, to then retrieve it using pandas.\nI used a guide for fetching and loading data from CoinMarketCap via API and loading that into a SQLite3 database.\nThe jupyter notebook can be viewed here:  \nAdditionally this is the code for the python script for fetching and loading the data.\nfrom requests import Request, Session\n\nfrom requests.exceptions import ConnectionError, Timeout, TooManyRedirects\n\nimport json\n\nimport sqlite3 as db\n\n  \n  \n\nurl = 'https://pro-api.coinmarketcap.com/v1/cryptocurrency/listings/latest'\n\nwith open(\"CMC_api.txt\") as f:\n\napi_key = f.read().strip()\n\n  \n\ndef fetch_data():\n\nparameters = {\n\n'convert':'USD',\n\n}\n\n  \n\nheaders = {\n\n'Accepts': 'application/json',\n\n'X-CMC_PRO_API_KEY': api_key,\n\n  \n\n}\n\n  \n\nsession = Session()\n\nsession.headers.update(headers)\n\n  \n\ntry:\n\nresponse = session.get(url, params=parameters)\n\ndata = json.loads(response.text)\n\nexcept (ConnectionError, Timeout, TooManyRedirects) as e:\n\nprint(e)\n\nreturn data\n\n  \n\ndef load_data():\n\ndata = fetch_data()\n\nconn = db.connect(\"crypto.db\")\n\ncn = conn.cursor()\n\ncn.execute(\"DROP TABLE IF EXISTS crypto_db\")\n\n  \n\ncreate_table_query = \"\"\"\n\nCREATE TABLE IF NOT EXISTS crypto_db\n\n(\n\nID INTEGER PRIMARY KEY AUTOINCREMENT,\n\n\"name\" VARCHAR(100),\n\n\"rank\" INTEGER,\n\n\"symbol\" VARCHAR(10),\n\n\"price_usd\" FLOAT\n\n)\n\n\"\"\"\n\n  \n\ncn.execute(create_table_query)\n\nfor item in data[\"data\"]:\n\ncn.execute(\"\"\"\n\nINSERT INTO crypto_db\n\n(\n\n\"name\",\n\n\"rank\",\n\n\"symbol\",\n\n\"price_usd\"\n\n)\n\nVALUES (?, ?, ?, ?)\n\n\"\"\", (\n\nitem[\"name\"],\n\nitem[\"cmc_rank\"],\n\nitem[\"symbol\"],\n\nitem[\"quote\"][\"USD\"][\"price\"],\n\n))\n\nconn.commit()\n\ncn.close()\n\nconn.close()\n\nprint(\"Loaded Data to SQLite3.\")\n\n  \n  \n\ndef run_etl() -&gt; None:\n\nload_data()\n\n  \n\nif __name__ == \"__main__\":\n\nrun_etl()"
  }
]