[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jon Skogland",
    "section": "",
    "text": "I’m a business intelligence analyst who’s in the process of learning programming and exploring topics in data analysis, data engineering and machine learning - to take my data skills to new heights.  I use this site to share my progress, thoughts, learnings and anything in between.  Below are my three most recent posts:\n\n\n\n\n\n 2025-08-15 - 27 - Greedy Cow Transport\n\n\n 2025-08-14 - 26 - Exploring dashboards in Quarto\n\n\n 2025-08-13 - 25 - Delhi PM2.5 (Normal distribution)\n\n\nNo matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "2025-08-15 - 27 - Greedy Cow Transport\n\n\n 2025-08-14 - 26 - Exploring dashboards in Quarto\n\n\n 2025-08-13 - 25 - Delhi PM2.5 (Normal distribution)\n\n\n 2025-08-12 - 24 - Random book picker\n\n\n 2025-08-11 - 23 - Classes practice (recreating simple pandas DataFrame)\n\n\n 2025-08-10 - Day 22 - Alder for optagne på uddannelse\n\n\n 2025-08-06 - Day 21 - Exploratory Data Analysis af adgangsgrundlag for vidergående uddannelser\n\n\n 2025-07-01 - Day 20 - Plotting practice\n\n\n 2025-06-30 - Day 19 - DecisionTreeRegressor\n\n\n 2025-06-29 - Day 18 - Pandas and Altair\n\n\n 2025-06-28 - Day 17 - Formula 1 API\n\n\n 2025-06-27 - Day 16 - Tracking of ISS on globe\n\n\n 2025-06-26 - Day 15 - Classification\n\n\n 2025-06-25 - Day 14 - Pandas exploration\n\n\n 2025-06-24 - Day 13 - Weather API\n\n\n 2025-06-22 - Day 12 - TicTacToe\n\n\n 2025-06-21 - Day 11 - Rock, Paper, Scissor\n\n\n 2025-06-20 - Day 10 - Retrieving by books by subject with API\n\n\n 2025-06-19 - Day 9 - Retrieving image with API\n\n\n 2025-06-18 - Day 8 - Dice roll generator\n\n\n 2025-06-17 - Day 7 - Price optimization\n\n\n 2025-06-16 - Day 6 - multivariate linear regression\n\n\n 2025-06-15 - Day 5 - Web scraping\n\n\n 2025-06-14 - Day 4 - API and SQL\n\n\n 2025-06-13 - Day 3 - data wrangling and plotting\n\n\n 2025-06-12 - Day 2 - Linear regression on big mac price vs GDP\n\n\n 2025-06-11 - Day 1 - Linear regression on numerical data\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Day 22 - Alder for optagne.html",
    "href": "posts/Day 22 - Alder for optagne.html",
    "title": "Day 22 - Alder for optagne på uddannelse",
    "section": "",
    "text": "Dataset hentet fra UFM.\nimport pandas as pd\nimport warnings\n\nwarnings.filterwarnings(\n    \"ignore\",\n    category=UserWarning,\n    module=\"openpyxl.styles.stylesheet\"\n)\ndf = pd.read_excel(\"data/day22/Reelt optagne_alder.xlsx\")\ndf.head(5)\n\n\n\n\n\n\n\n\nDen Koordinerede Tilmelding\nUnnamed: 1\nUnnamed: 2\nReelt optagne uanset prioritet fordelt på alder\nUnnamed: 4\nUnnamed: 5\nUnnamed: 6\nUnnamed: 7\nUnnamed: 8\nUnnamed: 9\nUnnamed: 10\nUnnamed: 11\nUnnamed: 12\nUnnamed: 13\nUnnamed: 14\n\n\n\n\n0\nNaN\nNaN\nNaN\n21-juli-2025\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nInstNr\nInstNavn\nOptNr\nOptNavn\nYngre\n19.0\n20.0\n21.0\n22.0\n23.0\n24.0\n25.0\n26-30\nÆldre\nI alt\n\n\n3\n1000\nKøbenhavns Universitet\n10110\nMedicin, København Ø, Studiestart: sommer- og ...\n26\n71.0\n110.0\n127.0\n81.0\n21.0\n18.0\n5.0\n12\n7\n478\n\n\n4\n1000\nKøbenhavns Universitet\n10112\nMedicin, Køge, Studiestart: sommer- og vinters...\n6\n15.0\n20.0\n26.0\n23.0\n14.0\n13.0\n3.0\n6\n8\n134\n# Fjerne 2 første rækker\ndf = df.iloc[2:,:]\n# Tilføje øverste række som kolonne navne\nny_header = df.iloc[0] # Gemme kolonnenavne\ndf = df.iloc[1:] # Fjerne øverste kolonne fra DataFrame\ndf.columns = ny_header\ndf.head(5)\n\n\n\n\n\n\n\n2\nInstNr\nInstNavn\nOptNr\nOptNavn\nYngre\n19.0\n20.0\n21.0\n22.0\n23.0\n24.0\n25.0\n26-30\nÆldre\nI alt\n\n\n\n\n3\n1000\nKøbenhavns Universitet\n10110\nMedicin, København Ø, Studiestart: sommer- og ...\n26\n71.0\n110.0\n127.0\n81.0\n21.0\n18.0\n5.0\n12\n7\n478\n\n\n4\n1000\nKøbenhavns Universitet\n10112\nMedicin, Køge, Studiestart: sommer- og vinters...\n6\n15.0\n20.0\n26.0\n23.0\n14.0\n13.0\n3.0\n6\n8\n134\n\n\n5\n1000\nKøbenhavns Universitet\n10115\nFolkesundhedsvidenskab, København K, Studiesta...\nNaN\n1.0\n10.0\n23.0\n24.0\n8.0\n1.0\nNaN\n1\nNaN\n68\n\n\n6\n1000\nKøbenhavns Universitet\n10117\nFarmaci, København Ø, Studiestart: sommerstart\n23\n42.0\n49.0\n51.0\n33.0\n18.0\n4.0\n2.0\n4\n2\n228\n\n\n7\n1000\nKøbenhavns Universitet\n10120\nOdontologi, København Ø, Studiestart: sommerstart\n13\n14.0\n16.0\n24.0\n23.0\n10.0\n5.0\n2.0\n8\n5\n120\n# Pivotere data fra wide til long format for nemmere dataanalyse\nmelted = pd.melt(df, id_vars=ny_header[:4], value_vars=ny_header[4:-1], var_name='Alder', value_name='Antal')\nmelted.head(5)\n\n\n\n\n\n\n\n\nInstNr\nInstNavn\nOptNr\nOptNavn\nAlder\nAntal\n\n\n\n\n0\n1000\nKøbenhavns Universitet\n10110\nMedicin, København Ø, Studiestart: sommer- og ...\nYngre\n26\n\n\n1\n1000\nKøbenhavns Universitet\n10112\nMedicin, Køge, Studiestart: sommer- og vinters...\nYngre\n6\n\n\n2\n1000\nKøbenhavns Universitet\n10115\nFolkesundhedsvidenskab, København K, Studiesta...\nYngre\nNaN\n\n\n3\n1000\nKøbenhavns Universitet\n10117\nFarmaci, København Ø, Studiestart: sommerstart\nYngre\n23\n\n\n4\n1000\nKøbenhavns Universitet\n10120\nOdontologi, København Ø, Studiestart: sommerstart\nYngre\n13\npd.set_option('future.no_silent_downcasting', True)\nmelted = melted.fillna(0).infer_objects(copy=False)\nmelted.dtypes\n\nInstNr        int64\nInstNavn     object\nOptNr         int64\nOptNavn      object\nAlder        object\nAntal       float64\ndtype: object\nmelted['Alder'] = melted['Alder'].astype(str)\nimport matplotlib.pyplot as plt\nimport numpy as np\nplt.bar(melted['Alder'], melted['Antal'])\nplt.xlabel(\"Alder\")\nplt.ylabel(\"Antal\")\nplt.title(\"Antal pr. Alder\")\nplt.xticks(rotation=45) \nplt.show()\nmelted['InstNavn'].unique()\n\narray(['Københavns Universitet',\n       'Copenhagen Business School - Handelshøjskolen',\n       'IT-Universitetet i København', 'Danmarks Tekniske Universitet',\n       'Roskilde Universitet', 'Syddansk Universitet',\n       'Aarhus Universitet', 'Aalborg Universitet',\n       'Det Kongelige Akademi - Arkitektur, Design, Konservering',\n       'Arkitektskolen Aarhus', 'Designskolen Kolding',\n       'Københavns Professionshøjskole', 'Professionshøjskolen Absalon',\n       'Professionshøjskolen UC Syddanmark',\n       'UCL Erhvervsakademi og Professionshøjskole',\n       'Professionshøjskolen VIA University College',\n       'Professionshøjskolen University College Nordjylland',\n       'Danmarks Medie- og Journalisthøjskole', 'Den Frie Lærerskole',\n       'Teknika – Copenhagen College of Technology Management and Marine Engineering',\n       'Svendborg International Maritime Academy, SIMAC',\n       'Fredericia Maskinmesterskole', 'Aarhus Maskinmesterskole',\n       'MARTEC - Maritime and Polytechnic University College',\n       'Erhvervsakademiet Copenhagen Business Academy',\n       'Københavns Erhvervsakademi (KEA)',\n       'Zealand Sjællands Erhvervsakademi', 'IBA Erhvervsakademi Kolding',\n       'Erhvervsakademi SydVest', 'Erhvervsakademi MidtVest',\n       'Erhvervsakademi Aarhus', 'Erhvervsakademi Dania'], dtype=object)\nkp = melted[melted['InstNavn'] == 'Københavns Professionshøjskole']\n# Gruppere kp DataFrame for alder for at kunne summere antal studerende korrekt\nkp = kp.groupby('Alder', as_index=False)['Antal'].sum()\n# Plot af alder for optagne på KPs uddannelser\nplt.bar(kp['Alder'], kp['Antal'])\nplt.xlabel(\"Alder\")\nplt.ylabel(\"Antal\")\nplt.title(\"KP - alder optagne\")\nplt.xticks(rotation=45) \nplt.show()"
  },
  {
    "objectID": "posts/Day 22 - Alder for optagne.html#udregning-af-chance-for-at-være-under-19-år-eller-over-25-år-gammel",
    "href": "posts/Day 22 - Alder for optagne.html#udregning-af-chance-for-at-være-under-19-år-eller-over-25-år-gammel",
    "title": "Day 22 - Alder for optagne på uddannelse",
    "section": "Udregning af chance for at være under 19 år eller over 25 år gammel",
    "text": "Udregning af chance for at være under 19 år eller over 25 år gammel\n\nyngre_antal = kp.loc[kp['Alder'] == 'Yngre', 'Antal'].sum()\n\n\nyngre_antal\n\nnp.float64(107.0)\n\n\n\nældre_antal = kp.loc[(kp['Alder'] == '26-30') | (kp['Alder'] == 'Ældre'), 'Antal'].sum()\n\n\nældre_antal\n\nnp.float64(1002.0)\n\n\n\np_yngre = yngre_antal / kp['Antal'].sum()\n\n\np_yngre\n\nnp.float64(0.023165187269971854)\n\n\n\np_over25 = ældre_antal /  kp['Antal'].sum()\n\n\np_over25\n\nnp.float64(0.2169300714440355)\n\n\nAltså er der næsten 10 gange større chance for at en person er over 25 end vedkommende er under 19"
  },
  {
    "objectID": "posts/24 - random book picker.html",
    "href": "posts/24 - random book picker.html",
    "title": "24 - Random book picker",
    "section": "",
    "text": "1 Goal\nToday my goal is to write a python program that selects books that I haven’t read from my Goodreads list, at random with filtering options. It should be possible to choose publishing date, max amount of pages, min rating, date added and maybe others.\n\nimport pandas as pd\n\n\ndf = pd.read_csv('data/day24/goodreads_library_export.csv')\n\n\ndf.head(5)\n\n\n\n\n\n\n\n\nBook Id\nTitle\nAuthor\nAuthor l-f\nAdditional Authors\nISBN\nISBN13\nMy Rating\nAverage Rating\nPublisher\n...\nDate Read\nDate Added\nBookshelves\nBookshelves with positions\nExclusive Shelf\nMy Review\nSpoiler\nPrivate Notes\nRead Count\nOwned Copies\n\n\n\n\n0\n51648276\nDrive Your Plow Over the Bones of the Dead\nOlga Tokarczuk\nTokarczuk, Olga\nAntonia Lloyd-Jones, Beata Poźniak\n=\"\"\n=\"\"\n0\n3.94\nPenguin Audio\n...\nNaN\n2023/11/08\nNaN\nNaN\nread\nNaN\nNaN\nNaN\n1\n0\n\n\n1\n18112493\nParissyndromet\nHeidi Furre\nFurre, Heidi\nNaN\n=\"8282880035\"\n=\"9788282880039\"\n0\n4.12\nFlamme\n...\nNaN\n2024/12/21\nNaN\nNaN\nread\nNaN\nNaN\nNaN\n1\n0\n\n\n2\n25489025\nThe Vegetarian\nHan Kang\nKang, Han\nDeborah Smith\n=\"0553448188\"\n=\"9780553448184\"\n0\n3.64\nHogarth\n...\nNaN\n2024/12/21\nNaN\nNaN\nread\nNaN\nNaN\nNaN\n1\n0\n\n\n3\n28921\nThe Remains of the Day\nKazuo Ishiguro\nIshiguro, Kazuo\nNaN\n=\"\"\n=\"\"\n0\n4.14\nFaber & Faber\n...\nNaN\n2025/07/15\nNaN\nNaN\nread\nNaN\nNaN\nNaN\n1\n0\n\n\n4\n43868109\nEmpire of Pain: The Secret History of the Sack...\nPatrick Radden Keefe\nKeefe, Patrick Radden\nNaN\n=\"0385545681\"\n=\"9780385545686\"\n0\n4.54\nDoubleday\n...\nNaN\n2025/07/10\nto-read\nto-read (#298)\nto-read\nNaN\nNaN\nNaN\n0\n0\n\n\n\n\n5 rows × 24 columns\n\n\n\n\n\n2 Data Cleaning\nFirst, I need to clean the data a little and remove unwanted columns and rows\n\n# Remove read books\nto_read = df[df['Read Count'] == 0]\n\n\nto_read\n\n\n\n\n\n\n\n\nBook Id\nTitle\nAuthor\nAuthor l-f\nAdditional Authors\nISBN\nISBN13\nMy Rating\nAverage Rating\nPublisher\n...\nDate Read\nDate Added\nBookshelves\nBookshelves with positions\nExclusive Shelf\nMy Review\nSpoiler\nPrivate Notes\nRead Count\nOwned Copies\n\n\n\n\n4\n43868109\nEmpire of Pain: The Secret History of the Sack...\nPatrick Radden Keefe\nKeefe, Patrick Radden\nNaN\n=\"0385545681\"\n=\"9780385545686\"\n0\n4.54\nDoubleday\n...\nNaN\n2025/07/10\nto-read\nto-read (#298)\nto-read\nNaN\nNaN\nNaN\n0\n0\n\n\n5\n40163119\nSay Nothing: A True Story of Murder and Memory...\nPatrick Radden Keefe\nKeefe, Patrick Radden\nNaN\n=\"0385521316\"\n=\"9780385521314\"\n0\n4.47\nDoubleday\n...\nNaN\n2025/07/10\nto-read\nto-read (#297)\nto-read\nNaN\nNaN\nNaN\n0\n0\n\n\n6\n42683\nOn Writing\nErnest Hemingway\nHemingway, Ernest\nLarry W. Phillips, Charles Scribner Jr.\n=\"0684854295\"\n=\"9780684854298\"\n0\n4.02\nScribner\n...\nNaN\n2025/06/12\nto-read\nto-read (#296)\nto-read\nNaN\nNaN\nNaN\n0\n0\n\n\n7\n22816087\nSeveneves\nNeal Stephenson\nStephenson, Neal\nNaN\n=\"\"\n=\"\"\n0\n4.00\nWilliam Morrow\n...\nNaN\n2025/06/11\nto-read\nto-read (#295)\nto-read\nNaN\nNaN\nNaN\n0\n0\n\n\n8\n50365\nA Suitable Boy (A Bridge of Leaves, #1)\nVikram Seth\nSeth, Vikram\nNaN\n=\"0060786523\"\n=\"9780060786526\"\n0\n4.11\nHarper Perennial Modern Classics\n...\nNaN\n2025/06/11\nto-read\nto-read (#294)\nto-read\nNaN\nNaN\nNaN\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n391\n28815\nInfluence: The Psychology of Persuasion\nRobert B. Cialdini\nCialdini, Robert B.\nNaN\n=\"006124189X\"\n=\"9780061241895\"\n0\n4.22\nHarper Business\n...\nNaN\n2018/08/27\nto-read\nto-read (#5)\nto-read\nNaN\nNaN\nNaN\n0\n0\n\n\n394\n2255\nWay of the Peaceful Warrior: A Book That Chang...\nDan Millman\nMillman, Dan\nNaN\n=\"1932073205\"\n=\"9781932073201\"\n0\n4.13\nHJ Kramer\n...\nNaN\n2018/08/27\nto-read\nto-read (#4)\nto-read\nNaN\nNaN\nNaN\n0\n0\n\n\n396\n19795\nPower vs. Force: The Hidden Determinants of Hu...\nDavid R. Hawkins\nHawkins, David R.\nNaN\n=\"1561709336\"\n=\"9781561709335\"\n0\n4.15\nHay House\n...\nNaN\n2018/08/21\nto-read\nto-read (#3)\nto-read\nNaN\nNaN\nNaN\n0\n0\n\n\n404\n566259\nFire in the Belly: On Being a Man\nSam Keen\nKeen, Sam\nNaN\n=\"0553351370\"\n=\"9780553351378\"\n0\n3.81\nBantam\n...\nNaN\n2018/08/21\nto-read\nto-read (#2)\nto-read\nNaN\nNaN\nNaN\n0\n0\n\n\n405\n1052\nThe Richest Man in Babylon\nGeorge S. Clason\nClason, George S.\nNaN\n=\"0451205367\"\n=\"9780451205360\"\n0\n4.23\nBerkley Books\n...\nNaN\n2018/08/21\nto-read\nto-read (#1)\nto-read\nNaN\nNaN\nNaN\n0\n0\n\n\n\n\n308 rows × 24 columns\n\n\n\n\ndf.columns\n\nIndex(['Book Id', 'Title', 'Author', 'Author l-f', 'Additional Authors',\n       'ISBN', 'ISBN13', 'My Rating', 'Average Rating', 'Publisher', 'Binding',\n       'Number of Pages', 'Year Published', 'Original Publication Year',\n       'Date Read', 'Date Added', 'Bookshelves', 'Bookshelves with positions',\n       'Exclusive Shelf', 'My Review', 'Spoiler', 'Private Notes',\n       'Read Count', 'Owned Copies'],\n      dtype='object')\n\n\n\n# Columns that I want to keep\ncolumns = ['Title', 'Author', 'Average Rating', 'Publisher',\n       'Number of Pages', 'Original Publication Year', 'Date Added']\n\nto_read = to_read[columns]\n\n\nto_read.head(5)\n\n\n\n\n\n\n\n\nTitle\nAuthor\nAverage Rating\nPublisher\nNumber of Pages\nOriginal Publication Year\nDate Added\n\n\n\n\n4\nEmpire of Pain: The Secret History of the Sack...\nPatrick Radden Keefe\n4.54\nDoubleday\n535.0\n2021.0\n2025/07/10\n\n\n5\nSay Nothing: A True Story of Murder and Memory...\nPatrick Radden Keefe\n4.47\nDoubleday\n441.0\n2018.0\n2025/07/10\n\n\n6\nOn Writing\nErnest Hemingway\n4.02\nScribner\n160.0\n1984.0\n2025/06/12\n\n\n7\nSeveneves\nNeal Stephenson\n4.00\nWilliam Morrow\n872.0\n2015.0\n2025/06/11\n\n\n8\nA Suitable Boy (A Bridge of Leaves, #1)\nVikram Seth\n4.11\nHarper Perennial Modern Classics\n1474.0\n1993.0\n2025/06/11\n\n\n\n\n\n\n\n\n# Remove NaN values\nto_read = to_read.dropna()\n\n\nto_read.head(5)\n\n\n\n\n\n\n\n\nTitle\nAuthor\nAverage Rating\nPublisher\nNumber of Pages\nOriginal Publication Year\nDate Added\n\n\n\n\n4\nEmpire of Pain: The Secret History of the Sack...\nPatrick Radden Keefe\n4.54\nDoubleday\n535.0\n2021.0\n2025/07/10\n\n\n5\nSay Nothing: A True Story of Murder and Memory...\nPatrick Radden Keefe\n4.47\nDoubleday\n441.0\n2018.0\n2025/07/10\n\n\n6\nOn Writing\nErnest Hemingway\n4.02\nScribner\n160.0\n1984.0\n2025/06/12\n\n\n7\nSeveneves\nNeal Stephenson\n4.00\nWilliam Morrow\n872.0\n2015.0\n2025/06/11\n\n\n8\nA Suitable Boy (A Bridge of Leaves, #1)\nVikram Seth\n4.11\nHarper Perennial Modern Classics\n1474.0\n1993.0\n2025/06/11\n\n\n\n\n\n\n\nI notice that some of the columns are type float, I want them to be integers instead\n\nto_read.dtypes\n\nTitle                         object\nAuthor                        object\nAverage Rating               float64\nPublisher                     object\nNumber of Pages              float64\nOriginal Publication Year    float64\nDate Added                    object\ndtype: object\n\n\n\nto_read = to_read.astype({'Number of Pages': int, 'Original Publication Year': int})\nto_read['Date Added'] = pd.to_datetime(to_read['Date Added'])\n\n\n\n3 Creating random book picker function\n\nimport datetime\nimport random\n\n\ndef random_book(df, options: int = 1, title: str = None, author: str = None, min_rating: float = 0, publisher: str = None, min_year: int = None, max_year: int = None, added_year: int = None, added_month: int = None): \n    if title is not None:\n        df = df.loc[df['Title'].str.contains(title, case=False)]\n        \n    if author is not None:\n        df = df.loc[df['Author'].str.contains(author, case=False)]\n        if df.empty == True:\n            print(\"You haven't saved any books that you want to read by that author\")\n            return\n        \n    if min_rating is not None and min_rating &gt;= df['Average Rating'].min():\n        df = df.loc[df['Average Rating'] &gt;= min_rating]\n        \n    if publisher is not None:\n        df = df.loc[df['Publisher'].str.contains(publisher)]\n        \n    if min_year is not None:\n        if min_year &lt; df['Original Publication Year'].min():\n            min_year = df['Original Publication Year'].min()\n        df = df.loc[df['Original Publication Year'] &gt;= min_year]\n        \n    if max_year is not None:\n        if max_year &gt; df['Original Publication Year'].max():\n            max_year = df['Original Publication Year'].max()\n        df = df.loc[df['Original Publication Year'] &lt;= max_year]\n        \n    if added_year is not None and (added_year &lt; df['Date Added'].dt.year.min() or added_year &gt; df['Date Added'].dt.year.max()):\n        df = df.loc[df['Date Added'].dt.year == added_year]\n        \n    if added_month is not None:\n        if (added_month &gt; 12 or added_month &lt; 1):\n            print('Month out of range, choose a number between 1 and 12')\n            return\n        df = df.loc[df['Date Added'].dt.month == added_month]\n\n    # Pick a book for the number of choices wanted\n    books = []\n    for i in range(options):\n        books.append(random.randint(0, len(df)-1))\n        \n    return df.iloc[books]\n       \n\n\n\n4 Testing\n\nrandom_book(to_read, added_month=2030)\n\nMonth out of range, choose a number between 1 and 12\n\n\n\nrandom_book(to_read, title='japan')\n\n\n\n\n\n\n\n\nTitle\nAuthor\nAverage Rating\nPublisher\nNumber of Pages\nOriginal Publication Year\nDate Added\n\n\n\n\n50\nBushido: The Soul of Japan\nInazō Nitobe\n3.84\nKodansha USA\n160\n1899\n2024-04-21\n\n\n\n\n\n\n\n\nrandom_book(to_read, min_year=1800, max_year=1940)\n\n\n\n\n\n\n\n\nTitle\nAuthor\nAverage Rating\nPublisher\nNumber of Pages\nOriginal Publication Year\nDate Added\n\n\n\n\n374\nThe Brothers Karamazov\nFyodor Dostoevsky\n4.39\nFarrar, Straus and Giroux\n796\n1880\n2018-11-10\n\n\n\n\n\n\n\n\nrandom_book(to_read, min_rating=4.1)\n\n\n\n\n\n\n\n\nTitle\nAuthor\nAverage Rating\nPublisher\nNumber of Pages\nOriginal Publication Year\nDate Added\n\n\n\n\n64\nMy Traitor's Heart: A South African Exile Retu...\nRian Malan\n4.25\nGrove Press\n349\n1990\n2023-04-05\n\n\n\n\n\n\n\n\nrandom_book(to_read, author='Murakami')\n\nYou haven't saved any books that you want to read by that author\n\n\n\nrandom_book(to_read, options=3)\n\n\n\n\n\n\n\n\nTitle\nAuthor\nAverage Rating\nPublisher\nNumber of Pages\nOriginal Publication Year\nDate Added\n\n\n\n\n32\nUtz\nBruce Chatwin\n3.67\nPenguin Publishing Group\n154\n1988\n2024-12-29\n\n\n321\nNine Chains to the Moon\nR. Buckminster Fuller\n3.85\nSouthern Illinois University Press\n384\n1963\n2020-10-19\n\n\n179\nSwann’s Way (In Search of Lost Time, #1)\nMarcel Proust\n4.16\nPenguin Classics\n468\n1913\n2021-09-10\n\n\n\n\n\n\n\n\n\n5 Conclusion\nThere we have it, a simple random book picker.\nIt however isn’t optimized for speed as I repeatedly re-assign the DataFrame instead of saving all the filters and then using the saved filtered in one filter operation for the dataframe.\nAlso the amount of parameters is high for the function, could be an option to use *arg and **kwargs instead.\nWould additionally have been better if there was an API for one’s own Goodreads library, then I wouldn’t have to download a csv file when new books are added. This was however just a for-fun coding task.\nAlso I’m lacking a ‘genre’ column, which would be nice to use to filter books by."
  },
  {
    "objectID": "posts/Day 8 - Python challenge.html",
    "href": "posts/Day 8 - Python challenge.html",
    "title": "Day 8 - Dice roll generator",
    "section": "",
    "text": "For lack of inspiration today, I solved a challenge put forth on the internet: https://www.reddit.com/r/dailyprogrammer/comments/8s0cy1/20180618_challenge_364_easy_create_a_dice_roller/\nThe goal was to create a dice rolling program that takes the input e.g. “2d10”, meaning rolling 2 dice of 10 sides. Whereby the program will return the cumulative sum of the dice rolls.\nThe jupyter notebook can be viewed here."
  },
  {
    "objectID": "posts/Day 7 - price optimization.html",
    "href": "posts/Day 7 - price optimization.html",
    "title": "Day 7 - Price optimization",
    "section": "",
    "text": "Tried my hand at price optimization - although heavily based on this guide: https://medium.com/operations-research-bit/a-practical-guide-to-pricing-optimisation-using-machine-learning-5ec4bf7f0d4c - as I have never done price optimization before.\nThe jupyter notebook can be viewed here."
  },
  {
    "objectID": "posts/Day 3 - data wrangling and plotting.html",
    "href": "posts/Day 3 - data wrangling and plotting.html",
    "title": "Day 3 - data wrangling and plotting",
    "section": "",
    "text": "Today I focused on using pandas and plotting using altair to improve those skills. The dataset of the day is again from the economist and can be viewed here\nToday was a bit easier compared to the previous days as they lack machine learning usage.\nThe jupyter notebook can be viewed here."
  },
  {
    "objectID": "posts/25 - Delhi PM2.5 (Normal distribution).html",
    "href": "posts/25 - Delhi PM2.5 (Normal distribution).html",
    "title": "25 - Delhi PM2.5 (Normal distribution)",
    "section": "",
    "text": "1 Goal\nToday I chose a dataset retrieved from Kaggle. With data regarding the air quality of Delhi, I want to try to create a normal distribution of some of the data\n\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.read_csv('data/day25/delhi_air_quality.csv')\n\n\ndf.head(5)\n\n\n\n\n\n\n\n\nDate\nMonth\nYear\nHolidays_Count\nDays\nPM2.5\nPM10\nNO2\nSO2\nCO\nOzone\nAQI\n\n\n\n\n0\n1\n1\n2021\n0\n5\n408.80\n442.42\n160.61\n12.95\n2.77\n43.19\n462\n\n\n1\n2\n1\n2021\n0\n6\n404.04\n561.95\n52.85\n5.18\n2.60\n16.43\n482\n\n\n2\n3\n1\n2021\n1\n7\n225.07\n239.04\n170.95\n10.93\n1.40\n44.29\n263\n\n\n3\n4\n1\n2021\n0\n1\n89.55\n132.08\n153.98\n10.42\n1.01\n49.19\n207\n\n\n4\n5\n1\n2021\n0\n2\n54.06\n55.54\n122.66\n9.70\n0.64\n48.88\n149\n\n\n\n\n\n\n\n\n# Get an understanding of the \ndf.describe()\n\n\n\n\n\n\n\n\nDate\nMonth\nYear\nHolidays_Count\nDays\nPM2.5\nPM10\nNO2\nSO2\nCO\nOzone\nAQI\n\n\n\n\ncount\n1461.000000\n1461.000000\n1461.000000\n1461.000000\n1461.000000\n1461.000000\n1461.000000\n1461.000000\n1461.000000\n1461.000000\n1461.000000\n1461.000000\n\n\nmean\n15.729637\n6.522930\n2022.501027\n0.189596\n4.000684\n90.774538\n218.219261\n37.184921\n20.104921\n1.025832\n36.338871\n202.210815\n\n\nstd\n8.803105\n3.449884\n1.118723\n0.392116\n2.001883\n71.650579\n129.297734\n35.225327\n16.543659\n0.608305\n18.951204\n107.801076\n\n\nmin\n1.000000\n1.000000\n2021.000000\n0.000000\n1.000000\n0.050000\n9.690000\n2.160000\n1.210000\n0.270000\n2.700000\n19.000000\n\n\n25%\n8.000000\n4.000000\n2022.000000\n0.000000\n2.000000\n41.280000\n115.110000\n17.280000\n7.710000\n0.610000\n24.100000\n108.000000\n\n\n50%\n16.000000\n7.000000\n2023.000000\n0.000000\n4.000000\n72.060000\n199.800000\n30.490000\n15.430000\n0.850000\n32.470000\n189.000000\n\n\n75%\n23.000000\n10.000000\n2024.000000\n0.000000\n6.000000\n118.500000\n297.750000\n45.010000\n26.620000\n1.240000\n45.730000\n284.000000\n\n\nmax\n31.000000\n12.000000\n2024.000000\n1.000000\n7.000000\n1000.000000\n1000.000000\n433.980000\n113.400000\n4.700000\n115.870000\n500.000000\n\n\n\n\n\n\n\nThus we see that there is four years of data available, with recordings everyday for those four years. It would now be interesting to plot the PM2.5 column.\n\ndf.columns\n\nIndex(['Date', 'Month', 'Year', 'Holidays_Count', 'Days', 'PM2.5', 'PM10',\n       'NO2', 'SO2', 'CO', 'Ozone', 'AQI'],\n      dtype='object')\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1461 entries, 0 to 1460\nData columns (total 12 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   Date            1461 non-null   int64  \n 1   Month           1461 non-null   int64  \n 2   Year            1461 non-null   int64  \n 3   Holidays_Count  1461 non-null   int64  \n 4   Days            1461 non-null   int64  \n 5   PM2.5           1461 non-null   float64\n 6   PM10            1461 non-null   float64\n 7   NO2             1461 non-null   float64\n 8   SO2             1461 non-null   float64\n 9   CO              1461 non-null   float64\n 10  Ozone           1461 non-null   float64\n 11  AQI             1461 non-null   int64  \ndtypes: float64(6), int64(6)\nmemory usage: 137.1 KB\n\n\n\nimport altair as alt\n\nalt.Chart(df).mark_point().encode(\n    x='Month',\n    y='PM2.5'\n)\n\n\n\n\n\n\n\nCan’t plot the PM2.5 column for whatever reason.\n\ndf = df.rename(columns={'PM2.5': 'PM2_5'})\n\n\n# Trying againg with the new column name\nalt.Chart(df).mark_point().encode(\n    x='Month',\n    y='PM2_5'\n)\n\n\n\n\n\n\n\nThat did the trick. We can clearly see that PM2.5 particals are generally lowest in July-September. With December and January being the worst. There is however an outlier in June with a PM2.5 of a 1000, maybe the instrument that measured couldn’t read above that threshold.\n\n\n2 Calculating the normal distribution for 2024 of PM2.5\n\nimport math\nimport matplotlib.pyplot as plt\n\n\ndf_2024 = df[df['Year'] == 2024]\n\n\ndef normal_pdf(x, mu=0, sigma=1):\n    sqrt_two_pi = math.sqrt(2 * math.pi)\n    return (math.exp(-(x-mu) ** 2 / 2 / sigma ** 2) / (sqrt_two_pi * sigma))\n\n\n# Storing the mean value of PM2.5 in 2024\nmu = df_2024['PM2_5'].mean()\n\n# Storing the standard deviation of PM2.5\nsigma = df_2024['PM2_5'].std()\n\n\n# Remove outlier at 1000 PM2_5\ndf_2024 = df_2024[df_2024['PM2_5'] &lt; df_2024['PM2_5'].quantile(0.99)]\n\n# Creating a array of continuous values to plot probability for each value. \n# As the pm2_5 column can't be used as-is, due to it missing values in the values between min and max\nxs = np.arange(min(df_2024['PM2_5']), max(df_2024['PM2_5']))\n\n# Storing y values of the function\ny = []\nfor x in xs:\n    y.append(normal_pdf(x, mu=mu, sigma=sigma))\n\n\n# plotting distribution\nplt.plot(xs, y)\nplt.title(\"Normal distribution of PM2.5 in Delhi 2024\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3 Reflections\nWe thus have a probability density distribution, where we can understand the probability of PM2.5 being any given value.\nBesides calculating the normal distribution, it could be interesting to use linear regression, to be able to approximate the pm2.5 on any given day."
  },
  {
    "objectID": "posts/27 - Greedy algorithm.html",
    "href": "posts/27 - Greedy algorithm.html",
    "title": "27 - Greedy Cow Transport",
    "section": "",
    "text": "This is my assignment solution to a part of problemset 1 from this course: Introduction to Computational Thinking and Data Science\n\n\n\n\n\ndef load_cows(filename):\n    cows = {}\n    with open(filename, 'r') as f: # open file\n        for line in f: # iterate through each line in txt file\n            name, tons = line.strip().split(\",\") # split line \n            cows[name] = int(tons) # Insert cow name and cow weight into dict\n    f.close\n    return cows\n        \n\n\ncows = load_cows('data/day27/ps1_cow_data.txt')\n\n\ncows\n\n{'Maggie': 3,\n 'Herman': 7,\n 'Betsy': 9,\n 'Oreo': 6,\n 'Moo Moo': 3,\n 'Milkshake': 2,\n 'Millie': 5,\n 'Lola': 2,\n 'Florence': 2,\n 'Henrietta': 9}\n\n\n\n\n\nResult of function should be a list of lists with name of the given cows that can be transported for each run. Each list in the list is a new run\n\ndef greedy_cow_transport(cows: dict, max_weight):\n    cows_copy = dict(sorted(cows.items(), key=lambda item: item[1], reverse=True))\n    transports = []\n    \n\n    while len(cows_copy) &gt; 0: # loop while the copy of cows is not empty\n        trip = [] # Initialize trip list\n        trip_weight = 0 # \n        for cow in list(cows_copy.keys()):              # iterate through a list of the keys in the cows dict (necessary to avoid problem with pop function below\n            if (trip_weight+cows_copy[cow]) &lt;= max_weight:              # Check if the total trip weight plus the current cow is less than allowed weight\n                trip.append(cow)                    # If the cow fits on the trip, add to trip\n                trip_weight += cows_copy[cow]       # add current cow to total weight on trip\n                cows_copy.pop(cow) # Remove current cow from the dictionary to not iterate through it again.\n        transports.append(trip) # when no more cows fit, append the current trip to transports lists \n    return transports   \n\nSolution\n\ngreedy_cow_transport(cows, 10)\n\n[['Betsy'],\n ['Henrietta'],\n ['Herman', 'Maggie'],\n ['Oreo', 'Moo Moo'],\n ['Millie', 'Milkshake', 'Lola'],\n ['Florence']]"
  },
  {
    "objectID": "posts/27 - Greedy algorithm.html#part-a-transporting-cows",
    "href": "posts/27 - Greedy algorithm.html#part-a-transporting-cows",
    "title": "27 - Greedy Cow Transport",
    "section": "",
    "text": "def load_cows(filename):\n    cows = {}\n    with open(filename, 'r') as f: # open file\n        for line in f: # iterate through each line in txt file\n            name, tons = line.strip().split(\",\") # split line \n            cows[name] = int(tons) # Insert cow name and cow weight into dict\n    f.close\n    return cows\n        \n\n\ncows = load_cows('data/day27/ps1_cow_data.txt')\n\n\ncows\n\n{'Maggie': 3,\n 'Herman': 7,\n 'Betsy': 9,\n 'Oreo': 6,\n 'Moo Moo': 3,\n 'Milkshake': 2,\n 'Millie': 5,\n 'Lola': 2,\n 'Florence': 2,\n 'Henrietta': 9}\n\n\n\n\n\nResult of function should be a list of lists with name of the given cows that can be transported for each run. Each list in the list is a new run\n\ndef greedy_cow_transport(cows: dict, max_weight):\n    cows_copy = dict(sorted(cows.items(), key=lambda item: item[1], reverse=True))\n    transports = []\n    \n\n    while len(cows_copy) &gt; 0: # loop while the copy of cows is not empty\n        trip = [] # Initialize trip list\n        trip_weight = 0 # \n        for cow in list(cows_copy.keys()):              # iterate through a list of the keys in the cows dict (necessary to avoid problem with pop function below\n            if (trip_weight+cows_copy[cow]) &lt;= max_weight:              # Check if the total trip weight plus the current cow is less than allowed weight\n                trip.append(cow)                    # If the cow fits on the trip, add to trip\n                trip_weight += cows_copy[cow]       # add current cow to total weight on trip\n                cows_copy.pop(cow) # Remove current cow from the dictionary to not iterate through it again.\n        transports.append(trip) # when no more cows fit, append the current trip to transports lists \n    return transports   \n\nSolution\n\ngreedy_cow_transport(cows, 10)\n\n[['Betsy'],\n ['Henrietta'],\n ['Herman', 'Maggie'],\n ['Oreo', 'Moo Moo'],\n ['Millie', 'Milkshake', 'Lola'],\n ['Florence']]"
  },
  {
    "objectID": "posts/Day 15 - Classification.html",
    "href": "posts/Day 15 - Classification.html",
    "title": "Day 15 - Classification",
    "section": "",
    "text": "Dataset was retrieved from here.\nSee jupyter notebook here."
  },
  {
    "objectID": "posts/Day 12 - TicTacToe.html",
    "href": "posts/Day 12 - TicTacToe.html",
    "title": "Day 12 - TicTacToe",
    "section": "",
    "text": "Today I created tictactoe - with limitations. I didn’t get around to test human input properly. You can’t win through diagonals yet… I know it is possible with numpy, but didn’t have the time. Also if it is a tie, the program keeps running.\nCode can be viewed here."
  },
  {
    "objectID": "posts/Day 20 - Plotting practice.html",
    "href": "posts/Day 20 - Plotting practice.html",
    "title": "Day 20 - Plotting practice",
    "section": "",
    "text": "Some plotting practice today.\nThe jupyter notebook can be viewed here"
  },
  {
    "objectID": "posts/Day 17 - Formula 1 API.html",
    "href": "posts/Day 17 - Formula 1 API.html",
    "title": "Day 17 - Formula 1 API",
    "section": "",
    "text": "Today I explored some Formula 1 data. Particularly i checked out the weather data for the Austrian Grandprix, which is happening this weekend.\nHere I become more familiar with datetime datatypes and their filtering. Link to code"
  },
  {
    "objectID": "posts/Day 2  - 1000 days of coding.html",
    "href": "posts/Day 2  - 1000 days of coding.html",
    "title": "Day 2 - Linear regression on big mac price vs GDP",
    "section": "",
    "text": "Today I used a datasetfrom The Economist of big mac prices and GDP across countries. I used this data to create a linear regression model to predict big mac prices based on GDP.\nAgain, like yesterday, I used Scikit-Learn and their linear regression model. As I’m still new to this space, I used guidance and code from this data science book.\nThe jupyter notebook can be viewed here."
  },
  {
    "objectID": "posts/Day 1 - 1000 days of coding.html",
    "href": "posts/Day 1 - 1000 days of coding.html",
    "title": "Day 1 - Linear regression on numerical data",
    "section": "",
    "text": "Today marks the first day of a thousand days of coding.\nHaven taken numerous certificates/courses in computer science and machine learning, I’ve come to the realisation (an obvious one at that) that nothing beats daily project based practice. Exercises from courses rarely cement themselves as it’s easy to merely follow instructions without thoroughly problemsolving.\nToday, day 1, I created a linear regression model on a dataset from Kaggle. Although it’s very minimal and not accurate at all, it is a good first.\nThe jupyter notebook can be viewed here."
  },
  {
    "objectID": "posts/Day 5 - Web scraping.html",
    "href": "posts/Day 5 - Web scraping.html",
    "title": "Day 5 - Web scraping",
    "section": "",
    "text": "Today I wanted to try my hand at web scraping to improve my data fetching skills.\nI did so by scraping all the name and addresses of Rema1000 stores across data and inserting them into a DataFrame.\nThe jupyter notebook can be viewed here."
  },
  {
    "objectID": "posts/Day 16 - Tracking of ISS on globe.html",
    "href": "posts/Day 16 - Tracking of ISS on globe.html",
    "title": "Day 16 - Tracking of ISS on globe",
    "section": "",
    "text": "See jupyter notebook here.\nI didn’t get around to clearning up the code and truly adding comments unfortunately. However, it gets the job done. I also wanted to make it automatic, so that the program would run every minute where you could slowly track the position of the ISS on the globe."
  },
  {
    "objectID": "posts/Day 6 - Multivariate Linear regression.html",
    "href": "posts/Day 6 - Multivariate Linear regression.html",
    "title": "Day 6 - multivariate linear regression",
    "section": "",
    "text": "Today I wanted to try multivariate linear regression\nI did so by using a dataset from the book Statistical Learning with Python, which included a few features of advertising expenditure in relation to sales figures.\nThe linear regression is however quite superficial, and does not go beyond fitting and evaluating model based on mean squared error, as I need to read through more text books on the subject to advance my machine learning skillset.\nThe jupyter notebook can be viewed here."
  },
  {
    "objectID": "posts/Day 4 - API and SQL.html",
    "href": "posts/Day 4 - API and SQL.html",
    "title": "Day 4 - API and SQL",
    "section": "",
    "text": "Today I wanted to try loading data from an API and inserting it into a database, to then retrieve it using pandas.\nI used a guide for fetching and loading data from CoinMarketCap via API and loading that into a SQLite3 database.\nThe jupyter notebook can be viewed here.\nAdditionally this is the code for the python script for fetching and loading the data.\nfrom requests import Request, Session\n\nfrom requests.exceptions import ConnectionError, Timeout, TooManyRedirects\n\nimport json\n\nimport sqlite3 as db\n\n  \n  \n\nurl = 'https://pro-api.coinmarketcap.com/v1/cryptocurrency/listings/latest'\n\nwith open(\"CMC_api.txt\") as f:\n\napi_key = f.read().strip()\n\n  \n\ndef fetch_data():\n\nparameters = {\n\n'convert':'USD',\n\n}\n\n  \n\nheaders = {\n\n'Accepts': 'application/json',\n\n'X-CMC_PRO_API_KEY': api_key,\n\n  \n\n}\n\n  \n\nsession = Session()\n\nsession.headers.update(headers)\n\n  \n\ntry:\n\nresponse = session.get(url, params=parameters)\n\ndata = json.loads(response.text)\n\nexcept (ConnectionError, Timeout, TooManyRedirects) as e:\n\nprint(e)\n\nreturn data\n\n  \n\ndef load_data():\n\ndata = fetch_data()\n\nconn = db.connect(\"crypto.db\")\n\ncn = conn.cursor()\n\ncn.execute(\"DROP TABLE IF EXISTS crypto_db\")\n\n  \n\ncreate_table_query = \"\"\"\n\nCREATE TABLE IF NOT EXISTS crypto_db\n\n(\n\nID INTEGER PRIMARY KEY AUTOINCREMENT,\n\n\"name\" VARCHAR(100),\n\n\"rank\" INTEGER,\n\n\"symbol\" VARCHAR(10),\n\n\"price_usd\" FLOAT\n\n)\n\n\"\"\"\n\n  \n\ncn.execute(create_table_query)\n\nfor item in data[\"data\"]:\n\ncn.execute(\"\"\"\n\nINSERT INTO crypto_db\n\n(\n\n\"name\",\n\n\"rank\",\n\n\"symbol\",\n\n\"price_usd\"\n\n)\n\nVALUES (?, ?, ?, ?)\n\n\"\"\", (\n\nitem[\"name\"],\n\nitem[\"cmc_rank\"],\n\nitem[\"symbol\"],\n\nitem[\"quote\"][\"USD\"][\"price\"],\n\n))\n\nconn.commit()\n\ncn.close()\n\nconn.close()\n\nprint(\"Loaded Data to SQLite3.\")\n\n  \n  \n\ndef run_etl() -&gt; None:\n\nload_data()\n\n  \n\nif __name__ == \"__main__\":\n\nrun_etl()"
  },
  {
    "objectID": "posts/Day 10 - Retrieving by books by subject with API.html",
    "href": "posts/Day 10 - Retrieving by books by subject with API.html",
    "title": "Day 10 - Retrieving by books by subject with API",
    "section": "",
    "text": "I used this API, to fetch books by subject. Here I created a simple function where it’s possible to insert the desired subject and print them in a dataframe.\nSee code below - can be viewed on github here\nimport requests\nimport pandas as pd\n\ndef retrieve_books_subject(subject):\n    # API URL\n    url = f\"https://openlibrary.org/subjects/{subject}\"\n    \n    params = {\n        \"details\" : \"true\" # In order to receive author information\n    }\n\n    headers = {\n        \"accept\": \"application/json\" # To specify json output format\n    }\n\n    # Get reponse to API\n    response = requests.get(url, headers=headers, params=params)\n    if response.status_code != 200:\n        return print(\"API call failed\")\n\n    # Store response as json format\n    data = response.json()\n    if int(data[\"work_count\"]) == 0:\n        return print(\"No books in this category. Please search for another category\")\n\n    # Initialize books variable for later use\n    books = []\n\n    # Iterate through json object to retrieve specific information (title and author)\n    for keys in data[\"works\"]:\n        # Store title of each book\n        title = keys[\"title\"]\n        authors = []\n        for key in keys[\"authors\"]:\n            # Append each author to a list\n            authors.append(key[\"name\"])\n\n        # Insert title and authors into books variable\n        books.append({\"title\" : title, \"authors\" : authors})\n\n    # Make list into dataframe\n    df_books = pd.DataFrame(books)\n\n    # Remove square brackets from authors columns\n    df_books[\"authors\"] = df_books[\"authors\"].apply(lambda x: \", \".join(x))\n\n    # Return dataframe of fetched books for display\n    return df_books"
  },
  {
    "objectID": "posts/23 - classes practice.html",
    "href": "posts/23 - classes practice.html",
    "title": "23 - Classes practice (recreating simple pandas DataFrame)",
    "section": "",
    "text": "import numpy as np\nfrom IPython.display import display, HTML\n\nRecreating the DataFrame class\n\nclass DataFrameCopy:\n    def __init__(self, data: dict):\n        self.data = data\n        self.create_index() # Run method to create index on initializing of object\n\n    # Add index to dataset\n    def create_index(self):\n        df = {} \n        max_rows = 0\n       \n        for key, value in self.data.items():  # return max number of rows to propagate index\n            if len(value) &gt; max_rows:\n                max_rows = len(value)\n        \n        index_length = list(range(0, max_rows)) # Create index column\n        df['index'] = index_length # Add index to temp dict\n        df.update(self.data) # Add initialize data to temp dict and reassign data that includes index\n        self.data = df\n\n    # Replicating the head function of pandas\n    def head(self, rows: int):\n        table = '&lt;table style=\"border-collapse: collapse; font-family: Arial; font-size: 14px;\"&gt;'\n\n        # loop to insert columns\n        table += '&lt;tr&gt;'\n        for key in self.data.keys():\n            table += f'&lt;th&gt;{key}&lt;/th&gt;'\n        table += '&lt;/tr&gt;'\n\n        # Below is the loop to insert rows values for each column\n        i = 0 # i is used to keep track of the row level, whereby\n        while True:\n            for key, value in self.data.items():\n                table += f\"&lt;td&gt;{value[i]}&lt;/td&gt;\"\n            table += \"&lt;/tr&gt;\"\n            i += 1\n\n            # When desired amount of rows have been inserted in html, break loop\n            if i == rows:\n               break\n        table += \"&lt;/table&gt;\"\n\n        # Read string as html\n        chart = HTML(table)\n        \n        return display(chart)\n       \n        \n        \n    \n\nInitializing DataFrameCopy class with data in the form of dictionary just like in pandas\n\ndata = {'Col 1': [1, 2, 3, 4, 5], 'Col 2': [6, 7, 8, 9, 10]}\ndf = DataFrameCopy(data)\n\nBelow the data from the object is called where we can see that the index has been automatically created\n\ndf.data\n\n{'index': [0, 1, 2, 3, 4], 'Col 1': [1, 2, 3, 4, 5], 'Col 2': [6, 7, 8, 9, 10]}\n\n\nUsing the head method for the class works simply like the pandas version, where we can choose the amount of rows we would like to show. The method then displays a html table of the data for improved readability.\n\ndf.head(4)\n\nindexCol 1Col 2016127238349"
  },
  {
    "objectID": "posts/Day 21 - Uddannelse.html",
    "href": "posts/Day 21 - Uddannelse.html",
    "title": "Day 21 - Exploratory Data Analysis af adgangsgrundlag for vidergående uddannelser",
    "section": "",
    "text": "Exploratory Data Analysis af adgangsgrundlag for videregående uddannelser\nData hentet fra UFM - Link\n\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.read_excel(\"data/day21/Ansøgere_adgangsgrundlag.xlsx\")\n\n/Users/jonskogland/Documents/hjemmeside - Jon Skogland/lokopu.github.io/venv/lib/python3.13/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n  warn(\"Workbook contains no default style, apply openpyxl's default\")\n\n\n\ndf\n\n\n\n\n\n\n\n\nDen Koordinerede Tilmelding\nUnnamed: 1\nUnnamed: 2\n1. prioritets-ansøgere fordelt på adgangsgrundlag\nUnnamed: 4\nUnnamed: 5\nUnnamed: 6\nUnnamed: 7\nUnnamed: 8\nUnnamed: 9\n...\nUnnamed: 11\nUnnamed: 12\nUnnamed: 13\nUnnamed: 14\nUnnamed: 15\nUnnamed: 16\nUnnamed: 17\nUnnamed: 18\nUnnamed: 19\nUnnamed: 20\n\n\n\n\n0\nNaN\nNaN\nNaN\n22-juli-2024\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nNaN\nNaN\nNaN\n1=stx\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nNaN\nNaN\nNaN\n2=hf\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\nNaN\n3=hhx\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n944\n8800\nErhvervsakademi Dania\n84410\nProduktionsteknolog, Randers SØ, Studiestart: ...\n6\nNaN\n1.0\n1.0\nNaN\n1.0\n...\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\n\n\n945\n8800\nErhvervsakademi Dania\n86160\nService- og oplevelsesøkonom, Randers SØ, Stud...\n15\n1\n7.0\n1.0\n3.0\nNaN\n...\nNaN\nNaN\n1.0\n1.0\nNaN\nNaN\nNaN\n1.0\nNaN\nNaN\n\n\n946\n8800\nErhvervsakademi Dania\n86164\nService- og oplevelsesøkonom, Randers SØ, E-læ...\n45\nNaN\n12.0\n10.0\n3.0\n4.0\n...\nNaN\nNaN\n5.0\n7.0\nNaN\n1.0\nNaN\n1.0\n1.0\n1.0\n\n\n947\n8800\nErhvervsakademi Dania\n86166\nService- og oplevelsesøkonom, Randers SØ, E-læ...\n9\nNaN\n1.0\n3.0\n4.0\nNaN\n...\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n948\n8800\nErhvervsakademi Dania\n87110\nLogistikøkonom , Hobro, Studiestart: sommerstart\n15\nNaN\n4.0\n5.0\n4.0\n1.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n949 rows × 21 columns\n\n\n\nHer ses det at dataframe’en har nogle header rækker øverst som skal filtreres fra, samtidigt er der data som skal bruges til at indsætte navne på adgangsgrundlagene for kolonnerne fra 4 og frem. da der ellers kun står et tal som er svært at læse.\n\nadgange = df.head(17)\n\n\nadgange = adgange.iloc[2:,3]\n\n\nadgange = adgange.str.split(\"=\", expand=True)\n\n\nadgange = adgange.rename(columns={0: \"id\", 1: \"adgangsgrundlag\"})\n\n\nadgange\n\n\n\n\n\n\n\n\nid\nadgangsgrundlag\n\n\n\n\n2\n1\nstx\n\n\n3\n2\nhf\n\n\n4\n3\nhhx\n\n\n5\n4\nhtx\n\n\n6\n5\nInternational Baccalaureate (IB)\n\n\n7\n6\nGIF\n\n\n8\n7\nAdgangseksamen for ingeniøruddannelser\n\n\n9\n8\nErhvervsuddannelse (EUD)\n\n\n10\n9\nAndet adgangsgrundlag\n\n\n11\n10\nInternational Baccalaureate (IB) fra udlandet\n\n\n12\n11\nAnden adgangsgivende eksamen fra ud-landet\n\n\n13\n12\neux\n\n\n14\n13\neux 1. del\n\n\n15\n14\nFærøsk gymnasial eksamen\n\n\n16\n15\nGrønlandsk gymnasial eksamen\n\n\n\n\n\n\n\n\ndf = df.iloc[18:]\n\n\nnew_header = df.iloc[0]\ndf = df[1:]\ndf.columns = new_header \n\nSå er hoveddataene fra excelarket korrekt afgrænset og det har fået de korrekte kolonner\n\ndf\n\n\n\n\n\n\n\n18\nInstNr\nInstNavn\nOptNr\nOptNavn\nI alt\nUoplyst\n1.0\n2.0\n3.0\n4.0\n...\n6.0\n7.0\n8.0\n9.0\n10.0\n11.0\n12.0\n13.0\n14.0\n15.0\n\n\n\n\n19\n1000\nKøbenhavns Universitet\n10110\nMedicin, København Ø, Studiestart: sommer- og ...\n1687\n4\n1107.0\n111.0\n14.0\n116.0\n...\n11.0\nNaN\n1.0\n10.0\n11.0\n240.0\n1.0\n2.0\n13.0\n1.0\n\n\n20\n1000\nKøbenhavns Universitet\n10112\nMedicin, Køge, Studiestart: sommer- og vinters...\n234\nNaN\n147.0\n25.0\n1.0\n38.0\n...\nNaN\nNaN\n2.0\n3.0\n1.0\n10.0\nNaN\nNaN\nNaN\nNaN\n\n\n21\n1000\nKøbenhavns Universitet\n10115\nFolkesundhedsvidenskab, København K, Studiesta...\n88\nNaN\n81.0\n3.0\nNaN\n1.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2.0\nNaN\nNaN\nNaN\n1.0\n\n\n22\n1000\nKøbenhavns Universitet\n10117\nFarmaci, København Ø, Studiestart: sommerstart\n291\nNaN\n194.0\n32.0\n2.0\n44.0\n...\n7.0\nNaN\nNaN\n3.0\nNaN\n4.0\nNaN\nNaN\n3.0\nNaN\n\n\n23\n1000\nKøbenhavns Universitet\n10120\nOdontologi, København Ø, Studiestart: sommerstart\n471\n1\n310.0\n46.0\n8.0\n54.0\n...\n8.0\nNaN\n1.0\n5.0\nNaN\n27.0\nNaN\nNaN\n7.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n944\n8800\nErhvervsakademi Dania\n84410\nProduktionsteknolog, Randers SØ, Studiestart: ...\n6\nNaN\n1.0\n1.0\nNaN\n1.0\n...\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\n\n\n945\n8800\nErhvervsakademi Dania\n86160\nService- og oplevelsesøkonom, Randers SØ, Stud...\n15\n1\n7.0\n1.0\n3.0\nNaN\n...\nNaN\nNaN\n1.0\n1.0\nNaN\nNaN\nNaN\n1.0\nNaN\nNaN\n\n\n946\n8800\nErhvervsakademi Dania\n86164\nService- og oplevelsesøkonom, Randers SØ, E-læ...\n45\nNaN\n12.0\n10.0\n3.0\n4.0\n...\nNaN\nNaN\n5.0\n7.0\nNaN\n1.0\nNaN\n1.0\n1.0\n1.0\n\n\n947\n8800\nErhvervsakademi Dania\n86166\nService- og oplevelsesøkonom, Randers SØ, E-læ...\n9\nNaN\n1.0\n3.0\n4.0\nNaN\n...\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n948\n8800\nErhvervsakademi Dania\n87110\nLogistikøkonom , Hobro, Studiestart: sommerstart\n15\nNaN\n4.0\n5.0\n4.0\n1.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n930 rows × 21 columns\n\n\n\nOvenstående dataframe er dog i wide format og det vil derfor være besværligt at lave analyse på. Derfor skal kolonnerne 1-15 pivoteres sådan at vi får to kolonner i stedet: “adgangsrundlag”, hvor værdierne vil være teksten på den uddannelse de studerende kommer fra, og “ansøgere” som vil være det antal som har søgt uddannelsen fra det givne adgangsgrundlag - denne vil erstatte “i alt” kolonnen da denne kan summeres efterfølgende.\n\nres = df.columns.tolist()\n\n\nres\n\n['InstNr',\n 'InstNavn',\n 'OptNr',\n 'OptNavn',\n 'I alt',\n 'Uoplyst',\n np.float64(1.0),\n np.float64(2.0),\n np.float64(3.0),\n np.float64(4.0),\n np.float64(5.0),\n np.float64(6.0),\n np.float64(7.0),\n np.float64(8.0),\n np.float64(9.0),\n np.float64(10.0),\n np.float64(11.0),\n np.float64(12.0),\n np.float64(13.0),\n np.float64(14.0),\n np.float64(15.0)]\n\n\n\nres = res[0:5]\n\n\nmelted = pd.melt(df, id_vars=res)\n\n\nmelted\n\n\n\n\n\n\n\n\nInstNr\nInstNavn\nOptNr\nOptNavn\nI alt\n18\nvalue\n\n\n\n\n0\n1000\nKøbenhavns Universitet\n10110\nMedicin, København Ø, Studiestart: sommer- og ...\n1687\nUoplyst\n4\n\n\n1\n1000\nKøbenhavns Universitet\n10112\nMedicin, Køge, Studiestart: sommer- og vinters...\n234\nUoplyst\nNaN\n\n\n2\n1000\nKøbenhavns Universitet\n10115\nFolkesundhedsvidenskab, København K, Studiesta...\n88\nUoplyst\nNaN\n\n\n3\n1000\nKøbenhavns Universitet\n10117\nFarmaci, København Ø, Studiestart: sommerstart\n291\nUoplyst\nNaN\n\n\n4\n1000\nKøbenhavns Universitet\n10120\nOdontologi, København Ø, Studiestart: sommerstart\n471\nUoplyst\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n14875\n8800\nErhvervsakademi Dania\n84410\nProduktionsteknolog, Randers SØ, Studiestart: ...\n6\n15.0\nNaN\n\n\n14876\n8800\nErhvervsakademi Dania\n86160\nService- og oplevelsesøkonom, Randers SØ, Stud...\n15\n15.0\nNaN\n\n\n14877\n8800\nErhvervsakademi Dania\n86164\nService- og oplevelsesøkonom, Randers SØ, E-læ...\n45\n15.0\n1.0\n\n\n14878\n8800\nErhvervsakademi Dania\n86166\nService- og oplevelsesøkonom, Randers SØ, E-læ...\n9\n15.0\nNaN\n\n\n14879\n8800\nErhvervsakademi Dania\n87110\nLogistikøkonom , Hobro, Studiestart: sommerstart\n15\n15.0\nNaN\n\n\n\n\n14880 rows × 7 columns\n\n\n\nNy mangler vi dog at “mappe” de forskelle adgangsgrundlag id’er sådan at der står det faktiske adgangsgrundlag og ikke tallet.\n\nd = {}\nfor index, row in adgange.iterrows():\n    d[float(row[\"id\"])] = row[\"adgangsgrundlag\"]\n\nd\n\n{1.0: 'stx',\n 2.0: 'hf',\n 3.0: 'hhx',\n 4.0: 'htx',\n 5.0: 'International Baccalaureate (IB)',\n 6.0: 'GIF',\n 7.0: 'Adgangseksamen for ingeniøruddannelser',\n 8.0: 'Erhvervsuddannelse (EUD)',\n 9.0: 'Andet adgangsgrundlag',\n 10.0: 'International Baccalaureate (IB) fra udlandet',\n 11.0: 'Anden adgangsgivende eksamen fra ud-landet',\n 12.0: 'eux',\n 13.0: 'eux 1. del',\n 14.0: 'Færøsk gymnasial eksamen',\n 15.0: 'Grønlandsk gymnasial eksamen'}\n\n\n\npd.set_option('future.no_silent_downcasting', False)\n#melted[18] = melted[18].replace(\"Uoplyst\", np.nan).infer_objects(copy=False)\n\n\nmelted[\"adgangsgrundlag\"] = melted[18].map(d)\n\n\nmelted = melted.drop(columns=18)\n\n\nmelted = melted.rename(columns={\"value\": \"antal\"})\n\n\nmelted[\"adgangsgrundlag\"] = melted[\"adgangsgrundlag\"].replace(np.nan, \"Uoplyst\").infer_objects(copy=False)\n\n\nmelted\n\n\n\n\n\n\n\n\nInstNr\nInstNavn\nOptNr\nOptNavn\nI alt\nantal\nadgangsgrundlag\n\n\n\n\n0\n1000\nKøbenhavns Universitet\n10110\nMedicin, København Ø, Studiestart: sommer- og ...\n1687\n4\nUoplyst\n\n\n1\n1000\nKøbenhavns Universitet\n10112\nMedicin, Køge, Studiestart: sommer- og vinters...\n234\nNaN\nUoplyst\n\n\n2\n1000\nKøbenhavns Universitet\n10115\nFolkesundhedsvidenskab, København K, Studiesta...\n88\nNaN\nUoplyst\n\n\n3\n1000\nKøbenhavns Universitet\n10117\nFarmaci, København Ø, Studiestart: sommerstart\n291\nNaN\nUoplyst\n\n\n4\n1000\nKøbenhavns Universitet\n10120\nOdontologi, København Ø, Studiestart: sommerstart\n471\n1\nUoplyst\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n14875\n8800\nErhvervsakademi Dania\n84410\nProduktionsteknolog, Randers SØ, Studiestart: ...\n6\nNaN\nGrønlandsk gymnasial eksamen\n\n\n14876\n8800\nErhvervsakademi Dania\n86160\nService- og oplevelsesøkonom, Randers SØ, Stud...\n15\nNaN\nGrønlandsk gymnasial eksamen\n\n\n14877\n8800\nErhvervsakademi Dania\n86164\nService- og oplevelsesøkonom, Randers SØ, E-læ...\n45\n1.0\nGrønlandsk gymnasial eksamen\n\n\n14878\n8800\nErhvervsakademi Dania\n86166\nService- og oplevelsesøkonom, Randers SØ, E-læ...\n9\nNaN\nGrønlandsk gymnasial eksamen\n\n\n14879\n8800\nErhvervsakademi Dania\n87110\nLogistikøkonom , Hobro, Studiestart: sommerstart\n15\nNaN\nGrønlandsk gymnasial eksamen\n\n\n\n\n14880 rows × 7 columns\n\n\n\n\n# Finde InstNavn for CBS\nmelted[\"InstNavn\"].unique()\n\narray(['Københavns Universitet',\n       'Copenhagen Business School - Handelshøjskolen',\n       'IT-Universitetet i København', 'Danmarks Tekniske Universitet',\n       'Roskilde Universitet', 'Syddansk Universitet',\n       'Aarhus Universitet', 'Aalborg Universitet',\n       'Det Kongelige Akademi - Arkitektur, Design, Konservering',\n       'Arkitektskolen Aarhus', 'Designskolen Kolding',\n       'Københavns Professionshøjskole', 'Professionshøjskolen Absalon',\n       'Professionshøjskolen UC Syddanmark',\n       'UCL Erhvervsakademi og Professionshøjskole',\n       'Professionshøjskolen VIA University College',\n       'Professionshøjskolen University College Nordjylland',\n       'Danmarks Medie- og Journalisthøjskole', 'Den Frie Lærerskole',\n       'Maskinmesterskolen København',\n       'Svendborg International Maritime Academy, SIMAC',\n       'Fredericia Maskinmesterskole', 'Aarhus Maskinmesterskole',\n       'MARTEC - Maritime and Polytechnic University College',\n       'Erhvervsakademiet Copenhagen Business Academy',\n       'Københavns Erhvervsakademi (KEA)',\n       'Zealand Sjællands Erhvervsakademi', 'IBA Erhvervsakademi Kolding',\n       'Erhvervsakademi SydVest', 'Erhvervsakademi MidtVest',\n       'Erhvervsakademi Aarhus', 'Erhvervsakademi Dania'], dtype=object)\n\n\n\n# Filtrere CBS\nCBS = melted[melted[\"InstNavn\"] == \"Copenhagen Business School - Handelshøjskolen\"]\n\n\nCBS\n\n\n\n\n\n\n\n\nInstNr\nInstNavn\nOptNr\nOptNavn\nI alt\nantal\nadgangsgrundlag\n\n\n\n\n75\n1300\nCopenhagen Business School - Handelshøjskolen\n13010\nErhvervsøkonomi, HA, Frederiksberg, Studiestar...\n925\nNaN\nUoplyst\n\n\n76\n1300\nCopenhagen Business School - Handelshøjskolen\n13015\nErhvervsøkonomi-informationsteknologi, HA (it....\n223\nNaN\nUoplyst\n\n\n77\n1300\nCopenhagen Business School - Handelshøjskolen\n13020\nErhvervsøkonomi-erhvervsret, HA (jur.), Freder...\n305\nNaN\nUoplyst\n\n\n78\n1300\nCopenhagen Business School - Handelshøjskolen\n13023\nErhvervsøkonomi og markeds- og kulturanalyse, ...\n251\nNaN\nUoplyst\n\n\n79\n1300\nCopenhagen Business School - Handelshøjskolen\n13025\nErhvervsøkonomi-matematik, HA (mat.), Frederik...\n89\nNaN\nUoplyst\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n14039\n1300\nCopenhagen Business School - Handelshøjskolen\n13085\nBusiness, Asian Language and Culture - Interna...\n71\nNaN\nGrønlandsk gymnasial eksamen\n\n\n14040\n1300\nCopenhagen Business School - Handelshøjskolen\n13090\nBusiness, Asian Language and Culture - Interna...\n72\nNaN\nGrønlandsk gymnasial eksamen\n\n\n14041\n1300\nCopenhagen Business School - Handelshøjskolen\n13100\nBusiness Administration and Digital Management...\n601\nNaN\nGrønlandsk gymnasial eksamen\n\n\n14042\n1300\nCopenhagen Business School - Handelshøjskolen\n13280\nInternational Shipping and Trade, Frederiksber...\n272\nNaN\nGrønlandsk gymnasial eksamen\n\n\n14043\n1300\nCopenhagen Business School - Handelshøjskolen\n13290\nEuropæisk business, HA i europæisk business, F...\n273\nNaN\nGrønlandsk gymnasial eksamen\n\n\n\n\n304 rows × 7 columns\n\n\n\nTilføjelse af procent kolonne\n\nimport altair as alt\n\n\n# lave ny kolonnen med procent af total med givne adgangsgrundlag\nmelted[\"procent\"] = (melted[\"antal\"] / melted[\"I alt\"])\n\n\nmelted.dtypes\n\nInstNr             object\nInstNavn           object\nOptNr              object\nOptNavn            object\nI alt              object\nantal              object\nadgangsgrundlag    object\nprocent            object\ndtype: object\n\n\n\nmelted['procent'].astype('float')\n\n0        0.002371\n1             NaN\n2             NaN\n3             NaN\n4        0.002123\n           ...   \n14875         NaN\n14876         NaN\n14877    0.022222\n14878         NaN\n14879         NaN\nName: procent, Length: 14880, dtype: float64\n\n\n\ncbs_stx = melted[(melted['InstNr']== 1300) & (melted['adgangsgrundlag'] == 'stx')]\n\n\nalt.Chart(cbs_stx).mark_point().encode(\n    y=alt.Y('OptNavn', axis=alt.Axis(labelAngle=0, labelLimit=0, titlePadding=400)),\n    x=\"procent\"\n)\n\n\n\n\n\n\n\nHer kan man altså se procentandelen af studerende for hver CBS Bachelor uddannelse, hvor den studerende har STX som adgangsgrundlag. Her giver det mening at de uddannelse med en STX procent på under 0.3 er på engelsk, hvor er vil være mange udenlandske studerende som derfor ikke har gået på STX. De engelske uddannelse ville give mening at analysere ved at se procentandelen af STX hvor udenlandske studerende er talt fra for at få en ide om hvilke grundlag de danske studerende har.\n\ncbs = melted[melted['InstNr'] == 1300]\n\n\npd.set_option('future.no_silent_downcasting', True)\ncbs.loc[:, 'antal'] = cbs['antal'].fillna(0)\ncbs.loc[:, 'procent'] = cbs['procent'].fillna(0)\n\n\ncbs\n\n\n\n\n\n\n\n\nInstNr\nInstNavn\nOptNr\nOptNavn\nI alt\nantal\nadgangsgrundlag\nprocent\n\n\n\n\n75\n1300\nCopenhagen Business School - Handelshøjskolen\n13010\nErhvervsøkonomi, HA, Frederiksberg, Studiestar...\n925\n0\nUoplyst\n0\n\n\n76\n1300\nCopenhagen Business School - Handelshøjskolen\n13015\nErhvervsøkonomi-informationsteknologi, HA (it....\n223\n0\nUoplyst\n0\n\n\n77\n1300\nCopenhagen Business School - Handelshøjskolen\n13020\nErhvervsøkonomi-erhvervsret, HA (jur.), Freder...\n305\n0\nUoplyst\n0\n\n\n78\n1300\nCopenhagen Business School - Handelshøjskolen\n13023\nErhvervsøkonomi og markeds- og kulturanalyse, ...\n251\n0\nUoplyst\n0\n\n\n79\n1300\nCopenhagen Business School - Handelshøjskolen\n13025\nErhvervsøkonomi-matematik, HA (mat.), Frederik...\n89\n0\nUoplyst\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n14039\n1300\nCopenhagen Business School - Handelshøjskolen\n13085\nBusiness, Asian Language and Culture - Interna...\n71\n0\nGrønlandsk gymnasial eksamen\n0\n\n\n14040\n1300\nCopenhagen Business School - Handelshøjskolen\n13090\nBusiness, Asian Language and Culture - Interna...\n72\n0\nGrønlandsk gymnasial eksamen\n0\n\n\n14041\n1300\nCopenhagen Business School - Handelshøjskolen\n13100\nBusiness Administration and Digital Management...\n601\n0\nGrønlandsk gymnasial eksamen\n0\n\n\n14042\n1300\nCopenhagen Business School - Handelshøjskolen\n13280\nInternational Shipping and Trade, Frederiksber...\n272\n0\nGrønlandsk gymnasial eksamen\n0\n\n\n14043\n1300\nCopenhagen Business School - Handelshøjskolen\n13290\nEuropæisk business, HA i europæisk business, F...\n273\n0\nGrønlandsk gymnasial eksamen\n0\n\n\n\n\n304 rows × 8 columns\n\n\n\n\nalt.Chart(cbs).mark_bar().encode(\n    y=alt.Y('OptNavn', axis=alt.Axis(labelAngle=0, labelLimit=0, titlePadding=400)),\n    x=\"procent\",\n    color='adgangsgrundlag'\n)\n\n\n\n\n\n\n\nHer kan vi se fordelingen af adgangsgrundlag mere tydeligt. Hvor STX og HHX dominerer på de danske uddannelser hos CBS.\nTil en videre analyse, ville det være interessant at sammenligne fordelingen med procentandelen af adgangsgrundlagene for STX, HHX osv. for at tyde om HHX’ere i højere grad søger CBS end STX relativt til antal studerende. Desuden at sammenligne med andre erhvervsøkonomiske uddannelser og se hvorledes deres fordelinger er."
  },
  {
    "objectID": "posts/Day 9 - Retrieving image with API.html",
    "href": "posts/Day 9 - Retrieving image with API.html",
    "title": "Day 9 - Retrieving image with API",
    "section": "",
    "text": "I initially wanted to fetch all the youtube videos from my favourite yoga youtuber, Tim Senesi, however the Google API proved too difficult at this time to be able to retrieve data from youtube.\nInstead I created a simple program to fetch and download the astronomy picture of the day from NASA.\nThe jupyter notebook can be viewed here.\n\nCode from program:\n# Libraries\n\nimport requests\n\nfrom dotenv import load_dotenv\n\nimport os\n  \n\n# Load dot environment files\n\nload_dotenv()\n\n\n# Get api key from environment file\n\nAPI_KEY = os.getenv(\"API_KEY\")\n\n# The url of Nasas image api\n\napi_url = \"https://api.nasa.gov/planetary/apod\"\n\n# Inserting api key when calling API\n\nparams = {\n\n\"api_key\": API_KEY\n\n}\n\n# Get response\n\nresponse = requests.get(api_url, params=params)\n\n# Convert response to json format\n\ndata = response.json()\n\n# Saving url of image to variable\n\nimage_url = data[\"hdurl\"]\n\n# Get request to fetch data from image url\n\nimg_data = requests.get(image_url).content\n\n\n# Open new file and write image data to file, where title of file is the title of the image\n\nwith open(f\"{data[\"title\"]}.jpg\", 'wb') as handler:\n\nhandler.write(img_data)"
  },
  {
    "objectID": "posts/Day 14 - Pandas exploration.html",
    "href": "posts/Day 14 - Pandas exploration.html",
    "title": "Day 14 - Pandas exploration",
    "section": "",
    "text": "Decided to dive deeper into pandas by exploration a data set from Kaggle.\nHere I learned to “explode” column to expand columns of lists and then remove leading whitespace to effectively count categorical values.\nSee jupyter notebook here."
  },
  {
    "objectID": "posts/Day 19 - DecisionTreeRegressor.html",
    "href": "posts/Day 19 - DecisionTreeRegressor.html",
    "title": "Day 19 - DecisionTreeRegressor",
    "section": "",
    "text": "I tried my hand at a decision tree regressor today using sklearn.\nThe jupyter notebook can be viewed here"
  },
  {
    "objectID": "posts/Day 18 - pandas.html",
    "href": "posts/Day 18 - pandas.html",
    "title": "Day 18 - Pandas and Altair",
    "section": "",
    "text": "I investigated some data from a “snapsegilde” as my friends group has a competition each year. Here I practiced my pandas and altair skills further.\nThe jupyter notebook can be viewed here"
  },
  {
    "objectID": "posts/Day 13 - weather api.html",
    "href": "posts/Day 13 - weather api.html",
    "title": "Day 13 - Weather API",
    "section": "",
    "text": "Used an API from Open-meteoto fetch weather data and create some charts to display the next few days forecast.\nSee jupyter notebook here."
  },
  {
    "objectID": "posts/Day 11 - Rock, paper, scissor.html",
    "href": "posts/Day 11 - Rock, paper, scissor.html",
    "title": "Day 11 - Rock, Paper, Scissor",
    "section": "",
    "text": "As I am short on time today, I created a simple rock, paper, scissor program in Python. It is not written robustly, but simply gets the job done.\nSee code here: https://github.com/JonSkogland/1000daysofcoding/blob/main/daily-projects/day11_20250621/day11.py"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Jon Skogland.\nThis is my personal page, where I’m going to share thoughts, learnings and things that I pursue.  Click the Posts link to see my posts  You can follow me here:\n\nMy Github: @lokopu"
  }
]